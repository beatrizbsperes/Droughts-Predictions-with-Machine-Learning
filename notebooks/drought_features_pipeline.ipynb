{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa79c13",
   "metadata": {},
   "source": [
    "\n",
    "# Drought Features: Weekly Aggregation for FIPS 6107\n",
    "\n",
    "This notebook builds a **feature dataset for drought modeling** by aggregating weather variables over rolling windows, using only the **weekly-scored dates** as targets.\n",
    "\n",
    "## What it does\n",
    "- Loads a CSV of daily (or sub-daily) weather data with a `score` column available on **weekly** rows.\n",
    "- Filters to **FIPS 6107**.\n",
    "- For every weekly (scored) date, computes rolling-window aggregates:\n",
    "  - **Precipitation** sums (7/30/90/180 days)\n",
    "  - **Temperature** means, plus **T2M_MAX** maxima\n",
    "  - **Humidity proxies** (QV2M, T2MDEW, T2MWET mean)\n",
    "  - **Wind** (10m, 50m) means/max/min-mean/range-mean\n",
    "  - **Pressure** means\n",
    "  - Additional derived metrics (precip deficits, temp anomalies, VPD proxies)\n",
    "- Exports a new dataset with one row per weekly (scored) date.\n",
    "\n",
    "> Tip: If you want to change the FIPS or add/remove features, look for the **Parameters** and **Feature Engineering** sections below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7141e5",
   "metadata": {},
   "source": [
    "\n",
    "## Parameters\n",
    "Update these to point to your input data and desired output file name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012cb539",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- File paths ---\n",
    "INPUT_CSV = './data/archive/train_timeseries/train_timeseries.csv'        # <--- update to your file path\n",
    "OUTPUT_CSV = './data/generated_data/fips_6107_drought_features.csv'\n",
    "\n",
    "# --- Target FIPS ---\n",
    "TARGET_FIPS = 6107  # Kern County, CA (example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c593bb46",
   "metadata": {},
   "source": [
    "\n",
    "## Load and prepare the data\n",
    "- Reads the input CSV\n",
    "- Parses the `date` column to datetime\n",
    "- Filters to the target **FIPS**\n",
    "- Sorts by date to ensure correct rolling-window logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdccb52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Convert date to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Filter for TARGET_FIPS only\n",
    "df_filtered = df[df['fips'] == TARGET_FIPS].copy()\n",
    "\n",
    "# Sort by date to ensure proper rolling calculations\n",
    "df_filtered = df_filtered.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Identify rows with scores (weekly observations)\n",
    "scored_dates = df_filtered[df_filtered['score'].notna()]['date'].values\n",
    "\n",
    "print(f\"Total rows in input: {len(df):,}\")\n",
    "print(f\"Rows for FIPS {TARGET_FIPS}: {len(df_filtered):,}\")\n",
    "print(f\"Weekly (scored) dates found: {len(scored_dates):,}\")\n",
    "df_filtered.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2557f5",
   "metadata": {},
   "source": [
    "\n",
    "## Feature engineering on weekly (scored) dates\n",
    "For each **target (weekly)** date, compute aggregates over the trailing windows:\n",
    "- **7, 30, 90, 180 days** (some families only use a subset).\n",
    "We build a new table with one row per weekly date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12163c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the new dataset - start with scored weeks only\n",
    "result_rows = []\n",
    "\n",
    "for target_date in scored_dates:\n",
    "    target_date = pd.Timestamp(target_date)\n",
    "    \n",
    "    # Get the score for this week\n",
    "    score_value = df_filtered.loc[df_filtered['date'] == target_date, 'score'].values[0]\n",
    "    \n",
    "    # Create masks for different rolling windows ending on target_date (exclusive of the lower bound)\n",
    "    mask_7d = (df_filtered['date'] <= target_date) & (df_filtered['date'] > target_date - pd.Timedelta(days=7))\n",
    "    mask_30d = (df_filtered['date'] <= target_date) & (df_filtered['date'] > target_date - pd.Timedelta(days=30))\n",
    "    mask_90d = (df_filtered['date'] <= target_date) & (df_filtered['date'] > target_date - pd.Timedelta(days=90))\n",
    "    mask_180d = (df_filtered['date'] <= target_date) & (df_filtered['date'] > target_date - pd.Timedelta(days=180))\n",
    "    \n",
    "    # Initialize row with basic info\n",
    "    row = {'fips': TARGET_FIPS, 'date': target_date, 'score': score_value}\n",
    "    \n",
    "    # PRECIPITATION FEATURES - Sum over windows\n",
    "    row['prec_sum_7d'] = df_filtered.loc[mask_7d, 'PRECTOT'].sum()\n",
    "    row['prec_sum_30d'] = df_filtered.loc[mask_30d, 'PRECTOT'].sum()\n",
    "    row['prec_sum_90d'] = df_filtered.loc[mask_90d, 'PRECTOT'].sum()\n",
    "    row['prec_sum_180d'] = df_filtered.loc[mask_180d, 'PRECTOT'].sum()\n",
    "    \n",
    "    # TEMPERATURE FEATURES - Mean over windows\n",
    "    for period, mask in [('7d', mask_7d), ('30d', mask_30d), ('90d', mask_90d), ('180d', mask_180d)]:\n",
    "        row[f't2m_mean_{period}'] = df_filtered.loc[mask, 'T2M'].mean()\n",
    "        row[f't2m_max_mean_{period}'] = df_filtered.loc[mask, 'T2M_MAX'].mean()\n",
    "        row[f't2m_min_mean_{period}'] = df_filtered.loc[mask, 'T2M_MIN'].mean()\n",
    "        row[f't2m_range_mean_{period}'] = df_filtered.loc[mask, 'T2M_RANGE'].mean()\n",
    "        row[f'ts_mean_{period}'] = df_filtered.loc[mask, 'TS'].mean()\n",
    "    \n",
    "    # TEMPERATURE MAX for heatwave detection (7d and 30d only)\n",
    "    row['t2m_max_7d'] = df_filtered.loc[mask_7d, 'T2M_MAX'].max()\n",
    "    row['t2m_max_30d'] = df_filtered.loc[mask_30d, 'T2M_MAX'].max()\n",
    "    \n",
    "    # HUMIDITY PROXIES - Mean over 7/30/90d\n",
    "    for period, mask in [('7d', mask_7d), ('30d', mask_30d), ('90d', mask_90d)]:\n",
    "        row[f'qv2m_mean_{period}'] = df_filtered.loc[mask, 'QV2M'].mean()\n",
    "        row[f't2mdew_mean_{period}'] = df_filtered.loc[mask, 'T2MDEW'].mean()\n",
    "        row[f't2mwet_mean_{period}'] = df_filtered.loc[mask, 'T2MWET'].mean()\n",
    "    \n",
    "    # WIND FEATURES - Mean and Max over 7/30d (evaporative demand)\n",
    "    for period, mask in [('7d', mask_7d), ('30d', mask_30d)]:\n",
    "        # 10m wind\n",
    "        row[f'ws10m_mean_{period}'] = df_filtered.loc[mask, 'WS10M'].mean()\n",
    "        row[f'ws10m_max_{period}'] = df_filtered.loc[mask, 'WS10M_MAX'].max()\n",
    "        row[f'ws10m_min_mean_{period}'] = df_filtered.loc[mask, 'WS10M_MIN'].mean()\n",
    "        row[f'ws10m_range_mean_{period}'] = df_filtered.loc[mask, 'WS10M_RANGE'].mean()\n",
    "        \n",
    "        # 50m wind\n",
    "        row[f'ws50m_mean_{period}'] = df_filtered.loc[mask, 'WS50M'].mean()\n",
    "        row[f'ws50m_max_{period}'] = df_filtered.loc[mask, 'WS50M_MAX'].max()\n",
    "        row[f'ws50m_min_mean_{period}'] = df_filtered.loc[mask, 'WS50M_MIN'].mean()\n",
    "        row[f'ws50m_range_mean_{period}'] = df_filtered.loc[mask, 'WS50M_RANGE'].mean()\n",
    "    \n",
    "    # PRESSURE FEATURES - Mean over 7/30d\n",
    "    row['ps_mean_7d'] = df_filtered.loc[mask_7d, 'PS'].mean()\n",
    "    row['ps_mean_30d'] = df_filtered.loc[mask_30d, 'PS'].mean()\n",
    "    \n",
    "    # ADDITIONAL DROUGHT-RELEVANT FEATURES\n",
    "    # Precipitation deficit (compare recent to longer-term average)\n",
    "    row['prec_deficit_30v90d'] = row['prec_sum_30d'] - (row['prec_sum_90d'] / 3)\n",
    "    row['prec_deficit_7v30d'] = row['prec_sum_7d'] - (row['prec_sum_30d'] / 4.3)\n",
    "    \n",
    "    # Temperature anomaly (recent vs longer-term)\n",
    "    row['temp_anomaly_7v90d'] = row['t2m_mean_7d'] - row['t2m_mean_90d']\n",
    "    row['temp_anomaly_30v180d'] = row['t2m_mean_30d'] - row['t2m_mean_180d']\n",
    "    \n",
    "    # Vapor pressure deficit proxy (temperature - dewpoint)\n",
    "    row['vpd_proxy_7d'] = row['t2m_mean_7d'] - row['t2mdew_mean_7d']\n",
    "    row['vpd_proxy_30d'] = row['t2m_mean_30d'] - row['t2mdew_mean_30d']\n",
    "    \n",
    "    result_rows.append(row)\n",
    "\n",
    "# Create final dataframe\n",
    "df_drought_features = pd.DataFrame(result_rows)\n",
    "print(f\"Dataset created with {len(df_drought_features)} weekly observations\")\n",
    "print(f\"Total features: {len(df_drought_features.columns)}\")\n",
    "df_drought_features.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cf988f",
   "metadata": {},
   "source": [
    "\n",
    "## Export results\n",
    "Saves the engineered features to CSV for downstream modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4df553",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save to CSV\n",
    "df_drought_features.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"Saved features to: {OUTPUT_CSV}\")\n",
    "print(f\"Columns ({len(df_drought_features.columns)}):\")\n",
    "print(df_drought_features.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e776bad",
   "metadata": {},
   "source": [
    "\n",
    "## Quick QA checks\n",
    "Missing values, dtypes, and a preview.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7732ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"/nDataset info:\")\n",
    "print(df_drought_features.info())\n",
    "\n",
    "print(\"/nMissing values (total):\", df_drought_features.isnull().sum().sum())\n",
    "\n",
    "# Peek\n",
    "df_drought_features.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4f5aa6",
   "metadata": {},
   "source": [
    "\n",
    "## Notes & Next Steps\n",
    "- If the dataset is very large, consider vectorizing with rolling windows or using a time-index and `pd.Series.rolling` for speed.\n",
    "- You can generalize this notebook to loop over **multiple FIPS** and then concatenate results.\n",
    "- For modeling, consider normalizing/standardizing features and adding **lag** versions of key metrics.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
